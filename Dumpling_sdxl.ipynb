{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9794d0-688e-4c09-b04d-43d4b4b14777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import FileResponse\n",
    "from modal import (\n",
    "    Image,\n",
    "    Mount,\n",
    "    Secret,\n",
    "    Volume,\n",
    "    asgi_app,\n",
    "    enter,\n",
    "    method)\n",
    "\n",
    "### Build up the environment\n",
    "app = modal.App(name=\"dumpling-dreambooth-app\")\n",
    "\n",
    "image = Image.debian_slim(python_version=\"3.10\").pip_install(\n",
    "    \"accelerate==0.27.2\",\n",
    "    \"datasets~=2.13.0\",\n",
    "    \"ftfy~=6.1.0\",\n",
    "    \"gradio~=4.29.0\",\n",
    "    \"smart_open~=6.4.0\",\n",
    "    \"transformers~=4.38.1\",\n",
    "    \"torch~=2.2.0\",\n",
    "    \"torchvision~=0.16\",\n",
    "    \"triton~=2.2.0\",\n",
    "    \"peft==0.7.0\",\n",
    "    \"wandb==0.16.3\",)\n",
    "\n",
    "### Downloading scripts and installing a git repo with `run_commands`\n",
    "GIT_SHA = (\n",
    "    \"abd922bd0c43a504e47eca2ed354c3634bd00834\"  # specify the commit to fetch)\n",
    "\n",
    "image = (\n",
    "    image.apt_install(\"git\")\n",
    "    # Perform a shallow fetch of just the target `diffusers` commit, checking out\n",
    "    # the commit in the container's home directory, /root. Then install `diffusers`\n",
    "    .run_commands(\n",
    "        \"cd /root && git init .\",\n",
    "        \"cd /root && git remote add origin https://github.com/huggingface/diffusers\",\n",
    "        f\"cd /root && git fetch --depth=1 origin {GIT_SHA} && git checkout {GIT_SHA}\",\n",
    "        \"cd /root && pip install -e .\",\n",
    "    ))\n",
    "\n",
    "### Configuration with `dataclass`es\n",
    "with image.imports():\n",
    "    from fastapi import Response\n",
    "    import io\n",
    "    import os\n",
    "\n",
    "@dataclass\n",
    "class SharedConfig:\n",
    "    \"\"\"Configuration information shared across project components.\"\"\"\n",
    "    # The instance name is the \"proper noun\" we're teaching the model\n",
    "    instance_name: str = \"Dumpling\"\n",
    "    # That proper noun is usually a member of some class (person, bird),\n",
    "    # and sharing that information with the model helps it generalize better.\n",
    "    class_name: str = \"Little Black Cat with Golden Eyes\"\n",
    "    # identifier for pretrained models on Hugging Face\n",
    "    model_name: str = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "    vae_name: str = \"madebyollin/sdxl-vae-fp16-fix\"  # required for numerical stability in fp16\n",
    "\n",
    "### Downloading weights with `run_function`\n",
    "def download_models():\n",
    "    import torch\n",
    "    from diffusers import AutoencoderKL, DiffusionPipeline\n",
    "    from transformers.utils import move_cache\n",
    "\n",
    "    config = SharedConfig()\n",
    "\n",
    "    DiffusionPipeline.from_pretrained(\n",
    "        config.model_name,\n",
    "        vae=AutoencoderKL.from_pretrained(\n",
    "            config.vae_name, torch_dtype=torch.float16\n",
    "        ),\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    move_cache()\n",
    "\n",
    "image = image.run_function(download_models)\n",
    "\n",
    "### Storing data generated by our app with `modal.Volume`\n",
    "volume = Volume.from_name(\n",
    "    \"dreambooth-finetuning-volume-v9\", create_if_missing=True)\n",
    "MODEL_DIR = \"/model\"\n",
    "\n",
    "checkpoint_dir = None # i added this and below for automated inference by checkpoint\n",
    "@dataclass\n",
    "class CkptConfig:\n",
    "    \"\"\"Set checkpoint directory\"\"\"\n",
    "    checkpoint_dir: str = None\n",
    "\n",
    "### Load finetuning dataset\n",
    "def load_images(image_urls: list[str]) -> Path:\n",
    "    import PIL.Image\n",
    "    from smart_open import open\n",
    "\n",
    "    img_path = Path(\"/img\")\n",
    "\n",
    "    img_path.mkdir(parents=True, exist_ok=True)\n",
    "    for ii, url in enumerate(image_urls):\n",
    "        with open(url, \"rb\") as f:\n",
    "            image = PIL.Image.open(f)\n",
    "            image.save(img_path / f\"{ii}.png\")\n",
    "    print(f\"{ii + 1} images loaded\")\n",
    "\n",
    "    return img_path\n",
    "\n",
    "### Finetuning with Hugging Face ðŸ§¨ Diffusers and Accelerate\n",
    "USE_WANDB = True # you can turn it to False if not using Weights & Biases\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig(SharedConfig):\n",
    "    \"\"\"Configuration for the finetuning step.\"\"\"\n",
    "\n",
    "    # training prompt looks like `{PREFIX} {INSTANCE_NAME} the {CLASS_NAME} {POSTFIX}`\n",
    "    prefix: str = \"a photo of\"\n",
    "    postfix: str = \"\"\n",
    "\n",
    "    # locator for plaintext file with urls for images of target instance\n",
    "    instance_example_urls_file: str = str(\n",
    "        Path(__file__).parent / \"instance_example_urls.txt\"\n",
    "    )\n",
    "\n",
    "    # Hyperparameters/constants from the huggingface training example\n",
    "    resolution: int = 1024\n",
    "    train_batch_size: int = 4 #tried 2, dumpling had 5 legs\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    learning_rate: float = 4e-4 #tried 4e-6 pic results didn't look very good altho loss trend very similar to other runs\n",
    "    lr_scheduler: str = \"constant\"\n",
    "    lr_warmup_steps: int = 0\n",
    "    max_train_steps: int = 500 #80 before\n",
    "    checkpointing_steps: int = 10\n",
    "    seed: int = 117\n",
    "\n",
    "\n",
    "@app.function(\n",
    "    image=image,\n",
    "    gpu=\"A100\",  # fine-tuning is VRAM-heavy and requires an A100 GPU\n",
    "    volumes={MODEL_DIR: volume},  # stores fine-tuned model\n",
    "    timeout=3600,  # 30 minutes\n",
    "    secrets=[Secret.from_name(\"my-wandb-secret\")] if USE_WANDB else [],\n",
    ")\n",
    "\n",
    "def train(instance_example_urls):\n",
    "    import subprocess\n",
    "\n",
    "    from accelerate.utils import write_basic_config\n",
    "\n",
    "    config = TrainConfig()\n",
    "\n",
    "    # load data locally\n",
    "    img_path = load_images(instance_example_urls)\n",
    "\n",
    "    # set up hugging face accelerate library for fast training\n",
    "    write_basic_config(mixed_precision=\"fp16\")\n",
    "\n",
    "    # define the training prompt\n",
    "    instance_phrase = f\"{config.instance_name} the {config.class_name}\"\n",
    "    prompt = f\"{config.prefix} {instance_phrase} {config.postfix}\".strip()\n",
    "\n",
    "    # the model training is packaged as a script, so we have to execute it as a subprocess, which adds some boilerplate\n",
    "    def _exec_subprocess(cmd: list[str]):\n",
    "        \"\"\"Executes subprocess and prints log to terminal while subprocess is running.\"\"\"\n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "        )\n",
    "        with process.stdout as pipe:\n",
    "            for line in iter(pipe.readline, b\"\"):\n",
    "                line_str = line.decode()\n",
    "                print(f\"{line_str}\", end=\"\")\n",
    "\n",
    "        if exitcode := process.wait() != 0:\n",
    "            raise subprocess.CalledProcessError(exitcode, \"\\n\".join(cmd))\n",
    "\n",
    "    # run training -- see huggingface accelerate docs for details\n",
    "    print(\"launching dreambooth training script\")\n",
    "    _exec_subprocess(\n",
    "        [\n",
    "            \"accelerate\",\n",
    "            \"launch\",\n",
    "            \"examples/dreambooth/train_dreambooth_lora_sdxl.py\",\n",
    "            \"--mixed_precision=fp16\",  # half-precision floats most of the time for faster training\n",
    "            f\"--pretrained_model_name_or_path={config.model_name}\",\n",
    "            f\"--pretrained_vae_model_name_or_path={config.vae_name}\",  # required for numerical stability in fp16\n",
    "            f\"--instance_data_dir={img_path}\",\n",
    "            f\"--output_dir={MODEL_DIR}\",\n",
    "            f\"--instance_prompt={prompt}\",\n",
    "            f\"--resolution={config.resolution}\",\n",
    "            f\"--train_batch_size={config.train_batch_size}\",\n",
    "            f\"--gradient_accumulation_steps={config.gradient_accumulation_steps}\",\n",
    "            f\"--learning_rate={config.learning_rate}\",\n",
    "            f\"--lr_scheduler={config.lr_scheduler}\",\n",
    "            f\"--lr_warmup_steps={config.lr_warmup_steps}\",\n",
    "            f\"--max_train_steps={config.max_train_steps}\",\n",
    "            f\"--checkpointing_steps={config.checkpointing_steps}\",\n",
    "            f\"--seed={config.seed}\",  # increased reproducibility by seeding the RNG\n",
    "        ]\n",
    "        + (\n",
    "            [\n",
    "                \"--report_to=wandb\",\n",
    "                f\"--validation_prompt={prompt} in space\",  # simple test prompt\n",
    "                f\"--validation_epochs={config.max_train_steps // 10}\",\n",
    "            ]\n",
    "            if USE_WANDB\n",
    "            else []\n",
    "        ),\n",
    "    )\n",
    "    # The trained model information has been output to the volume mounted at `MODEL_DIR`.\n",
    "    # To persist this data for use in our web app, we 'commit' the changes\n",
    "    # to the volume.\n",
    "    volume.commit()\n",
    "\n",
    "### Running our model (train & subsequently inference)\n",
    "@app.local_entrypoint()\n",
    "def run():\n",
    "    with open(TrainConfig().instance_example_urls_file) as f:\n",
    "        instance_example_urls = [line.strip() for line in f.readlines()]\n",
    "    train.remote(instance_example_urls)\n",
    "\n",
    "@app.cls(image=image, gpu=\"A10G\", volumes={MODEL_DIR: volume})\n",
    "class Model:\n",
    "    @enter()\n",
    "    def load_model(self):\n",
    "        import torch\n",
    "        from diffusers import AutoencoderKL, DiffusionPipeline\n",
    "\n",
    "        config = TrainConfig()\n",
    "        ckpt = CkptConfig()\n",
    "        print(f\"checkpoint directory: {ckpt.checkpoint_dir}\")\n",
    "\n",
    "        # Reload the modal.Volume to ensure the latest state is accessible.\n",
    "        volume.reload()\n",
    "\n",
    "        # set up a hugging face inference pipeline using our model\n",
    "        pipe = DiffusionPipeline.from_pretrained(\n",
    "            config.model_name,\n",
    "            vae=AutoencoderKL.from_pretrained(\n",
    "                config.vae_name, torch_dtype=torch.float16\n",
    "            ),\n",
    "            torch_dtype=torch.float16,\n",
    "        ).to(\"cuda\")\n",
    "        pipe.load_lora_weights(ckpt.checkpoint_dir) # was pipe.load_lora_weights(MODEL_DIR)\n",
    "        self.pipe = pipe\n",
    "    \n",
    "    def _inference(self, text, num_inference_steps: int=25, guidance_scale: float=7.5):\n",
    "        image = self.pipe(\n",
    "            text,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=num_inference_steps,\n",
    "        ).images[0]\n",
    "        byte_stream=io.BytesIO()\n",
    "        image.save(byte_stream,format=\"JPEG\")\n",
    "        return byte_stream\n",
    "\n",
    "    @modal.web_endpoint(docs=True)\n",
    "    def web_inference(self, text:str, num_inference_steps: int=25, guidance_scale: float=7.5):\n",
    "        return Response(\n",
    "            content=self._inference(text, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale).getvalue(),\n",
    "            media_type=\"image/jpeg\",\n",
    "        )\n",
    "\n",
    "\n",
    "### To train, run the following command in terminal: \"modal run name_of_this_file.py\"\n",
    "### To inference, run: \"modal serve name_of_this_file.py\" then open up the URL link, add \\docs to test out different prompts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
